{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cpcTIDflIr0"
      },
      "source": [
        "# `Sentimental Analysis on IMBD Movie reviews with RNN`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEAoYr2Plbx_"
      },
      "source": [
        "We will build an LSTM Recurrent Neural Network to perform sentimental anaylsis on IMBD movie reviews to determine whether a certain review is positive or negative. We will be using the Tensorflow IMBD dataset & we will start by importing the necessary libraries and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tCP_Kt3lxCg"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNX-l9nZmS_1"
      },
      "source": [
        "This dataset contains reviews that are encoded numerically. The reviews are divided into words and each word in a review is encoded to a numerical value that represents the frequency of the word. VOCAB_SIZE will represent the amount of vocabulary used in the reviews, where we will use a set of 80000 words to select our reviews from. A word encoded by '1' will be the most frequent, while a word encoded by '80000' will be the least frequenct."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGcc1He0mOia",
        "outputId": "24485986-5fcd-44d1-fb16-cc760df10371"
      },
      "source": [
        "VOCAB_SIZE = 80000\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=VOCAB_SIZE)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJAwZsOAqVcs"
      },
      "source": [
        "train_data & test_data are two 2D array of equal size, where each entry in the outer array is an array containing the review. Every element in the array is a numerical value representing a word within the review. Each element in the array has a different size, since the reviews vary in the number of words. train_labels & test_labels are arrays of 1s and 0s. A one represents a positive review & a zero represents a negative review. Their values correspond to the numerical review arrays of train_data & test_data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00FXzCNasFuD"
      },
      "source": [
        "Since train_data & test_data consist of 2D array of varying size (since each review has a different amount of words), we must alter the size of each review in the array to make every element in the array equally sized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zAu62KwqCmj"
      },
      "source": [
        "def padding_data(data, size):\n",
        "  padded_data = np.empty((25000,size), int)\n",
        "  for i in range(0,len(data)):\n",
        "    if len(data[i]) > size:\n",
        "      padded_data[i] = data[i][0:size]\n",
        "    elif len(data[i]) < size:\n",
        "      zeros = np.zeros((size-len(data[i]),), dtype=int)\n",
        "      data_item = data[i][0:len(data[i])]\n",
        "      padded_data[i] = np.concatenate([zeros,data_item])\n",
        "    else: \n",
        "      padded_data[i] = data[i]\n",
        "  return padded_data"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMacCkGR55Z9"
      },
      "source": [
        "We create the function padding_data to readjust the size of each element in the train_data & test_data datasets into MAXLEN. If a review has more than MAXLEN words, the review will be trimmed to the first 'MAXLEN' words. If a review has less than MAXLEN words, then the review will be appended with (MAXLEN - review.size) number of 0s in order to make every entry in the array equally sized. The zeros are not representative of any word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoeP5vxU68jq"
      },
      "source": [
        "train_data = padding_data(train_data, MAXLEN)\n",
        "test_data = padding_data(test_data, MAXLEN)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6jtv_fO85_G"
      },
      "source": [
        "Now, we will build our Sequential model. The first layer we'll add is an embedding layer, where we will plug in our vocabulary size as an input dimension & 64 will be the dimension for each embedding output. The embedding layer will find meaning behind the positioning of a vocabulary word within a sentence. Next layer will be our LSTM (Long-Short Term Memory), where we will pass in 64 to represent the number of dimensions for each word, corresponding to the output dimension size used in the mebedding layer. Finally, we will add a dense layer to our NN. We will use a sigmoid function, since our expected output must be between either 1 (positive review) or 0 (negative review). We also add 1 unit for our dense layer, since we are expecting one output per review (whether the sentiment is positive or negative)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfTdCkFB7Itn"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 64),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhNw1UG4DY2k"
      },
      "source": [
        "Next, we will compile our model. We will use the binary cross-entropy as our loss function, due to the nature of our output variable (1 or 0). We will also use 'Adam' as our optimizer and accuracy for our metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0eV2cCkELRX"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ9v8BIGHpXJ"
      },
      "source": [
        "Now, we will train our model by passing the parameters for x & y as train_data and train_labels, epochs will represent the number of times the data is re-fed into the model and validation_split represents the ratio of data used for validating our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4P2rKysRHti0",
        "outputId": "83ff0c00-bb2f-4cdf-dd85-c1d3373d12ab"
      },
      "source": [
        "history = model.fit(train_data, train_labels, epochs = 10, validation_split = 0.2)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 115s 182ms/step - loss: 0.5616 - accuracy: 0.6907 - val_loss: 0.3793 - val_accuracy: 0.8460\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 115s 184ms/step - loss: 0.2203 - accuracy: 0.9212 - val_loss: 0.3586 - val_accuracy: 0.8624\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 115s 184ms/step - loss: 0.1024 - accuracy: 0.9659 - val_loss: 0.4691 - val_accuracy: 0.8392\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 114s 183ms/step - loss: 0.0611 - accuracy: 0.9798 - val_loss: 0.5074 - val_accuracy: 0.8574\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 114s 183ms/step - loss: 0.0452 - accuracy: 0.9868 - val_loss: 0.6074 - val_accuracy: 0.8584\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 114s 183ms/step - loss: 0.0275 - accuracy: 0.9909 - val_loss: 0.6025 - val_accuracy: 0.8516\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 114s 183ms/step - loss: 0.0255 - accuracy: 0.9920 - val_loss: 0.6402 - val_accuracy: 0.8518\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 115s 184ms/step - loss: 0.0617 - accuracy: 0.9778 - val_loss: 0.6726 - val_accuracy: 0.8482\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 116s 185ms/step - loss: 0.0114 - accuracy: 0.9962 - val_loss: 0.6785 - val_accuracy: 0.8384\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 116s 185ms/step - loss: 0.0163 - accuracy: 0.9949 - val_loss: 0.8880 - val_accuracy: 0.8436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKf8QUbJQWnK"
      },
      "source": [
        "Now, we will evaluate our model by checking the accuracy achieved on the test_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWrwoG5sQW3i",
        "outputId": "5b5ca79d-1b1c-4888-914e-1cee728625a9"
      },
      "source": [
        "test_error, test_accuracy = model.evaluate(test_data, test_labels)\n",
        "print(test_error)\n",
        "print(test_accuracy)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 25s 32ms/step - loss: 0.9610 - accuracy: 0.8303\n",
            "0.9610259532928467\n",
            "0.8302800059318542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VXSiLQcVJSN"
      },
      "source": [
        "We will now attempt to predict the sentiment of a movie review that the user can input into the program. In order to do so, we must encode the user input in order for the input to resemble the data. I will import sequence for padding the data, since 'padding_data' works on 2D arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et0MeDkwf2B7"
      },
      "source": [
        "from keras.preprocessing import sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQXOcr_vRSsh"
      },
      "source": [
        "# Get the list of words & their respective indeces (respective numerical values)\n",
        "word_idx = imdb.get_word_index()\n",
        "\n",
        "def encode_sentence(sentence):\n",
        "  tokens = tf.keras.preprocessing.text.text_to_word_sequence(sentence)\n",
        "  for i in range(0,len(tokens)):\n",
        "    if tokens[i] in word_idx:\n",
        "      tokens[i] = word_idx[tokens[i]]\n",
        "    else:\n",
        "      tokens[i] = 0\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDYtKQEXeG1o"
      },
      "source": [
        "The function encode_sentence will take in a sentence & turn each word within the sentence into it's numerical representation (turns every word into it's index in the word_idx list)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h39yzE-9gaIg"
      },
      "source": [
        "def predict_sentiment(sentence):\n",
        "  encoded_sentence = encode_sentence(sentence)\n",
        "  sentiment_prediction = np.zeros((1,250))\n",
        "  sentiment_prediction[0] = encoded_sentence\n",
        "  result = model.predict(sentiment_prediction[0])\n",
        "  if result[0] > 0.5:\n",
        "    print('Positive Sentiment')\n",
        "  elif result[0] < 0.5:\n",
        "    print('Negative Sentiment')\n",
        "  else: \n",
        "    print('Neutral Sentiment')"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAgzjaUTdzIB",
        "outputId": "2b26bbb9-dea7-4288-b0fa-0f9b5c40b018"
      },
      "source": [
        "sentence = 'This was a great movie I absolutely loved it because it exceeded all of my expectations I would definitely recommend it'\n",
        "predict_sentiment(sentence)"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}