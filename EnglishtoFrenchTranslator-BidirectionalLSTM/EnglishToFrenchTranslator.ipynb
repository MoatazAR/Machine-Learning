{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EnglishToFrenchTranslator.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv1cZV4rQSaq"
      },
      "source": [
        "# Bi-directional RNN Translator (encoder-decoder architecture)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDSZv0BPQt32"
      },
      "source": [
        "We will create a language translator using Recurrent BiDirectional LSTMs in the Tensorflow library of Python. We choose a RNN, since RNNs are the preferred model used for Natural Language Processing. BiDirectional LSTMs are an extension of LSTMs that can enhance a model's performance on sequential classification problems. LSTMs will preserve 'past' information corresponding to certain data and will store & use this information when processing current data. A BiDirectional LSTM will process data in both a backwards and forward direction, and it will store both 'past' and 'future' information relevant to the data, making it simpler for the model to detect patterns/sequences in either direction. This will be especially useful when processing speech, since it's easier to understand the meaning of a word when given the previous & next couple of words in the sentence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDV53VTruxhh"
      },
      "source": [
        "## Import libraries & datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy7h6N4ni7H5"
      },
      "source": [
        "First, we will begin by importing the necessary libraries & uploading our data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-bAB25yhp4w"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import LSTM, Bidirectional, Dense, Embedding, Input, TimeDistributed, Embedding"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Tig9Mgj7J2i"
      },
      "source": [
        "We will upload the data stored in google drive. We will subsequently convert the dataframe into a list of strings, each string resembling a sentence in it's resperctive dataframe's language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzZ8JhYhomrs",
        "outputId": "7b4f3ef6-b0bb-4ca1-8f1f-eab48acc251d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwI1CUMspOEu",
        "outputId": "ee69df70-be7e-4efc-ee55-2056a210fcde"
      },
      "source": [
        "df_eng = pd.read_csv('/content/drive/MyDrive/Data-files/small_vocab_en.csv', sep='delimiter', header=None)\n",
        "df_frn = pd.read_csv('/content/drive/MyDrive/Data-files/small_vocab_fr.csv', sep='delimiter', header=None)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EPENHb012u9"
      },
      "source": [
        "from itertools import chain\n",
        "english_sentences = df_eng.astype(str).values.tolist()\n",
        "english_sentences = list(chain.from_iterable(english_sentences))\n",
        "\n",
        "french_sentences = df_frn.astype(str).values.tolist()\n",
        "french_sentences = list(chain.from_iterable(french_sentences))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbDUkrPi-nJ5"
      },
      "source": [
        "In order to feed our data into the neural network, we must convert the input from strings into numbers by tokenizing our list. The tokenizer function will take in a list of sentences and will return a 2D list, where each sentence is converted into a list of number & each number represents a word. The smaller the number, the more frequent a word is. For example, the sentence 'The man walks his dog in the park' will be converted into [1,2,3,4,5,6,1,7], where 'the' is represented by 1 since it's the most frequent word present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxTykNIZu5XT"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9vFG-Vlw2xI"
      },
      "source": [
        "def tokenize(x):\n",
        "\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(x)\n",
        "    return tokenizer.texts_to_sequences(x), tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUd0GcOwDPHW",
        "outputId": "d94b641a-7225-43d2-b5b4-fa4cc4f9a715"
      },
      "source": [
        "english_numbers, english_tokenizer = tokenize(english_sentences)\n",
        "english_numbers[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17, 23, 1, 8, 67, 4, 39, 7, 3, 1, 55, 2, 44]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf6L4l4kDvDA"
      },
      "source": [
        "The sentence 'new jersey is sometimes quiet during autumn , and it is snowy in april .' is converted into the number list [17, 23, 1, 8, 67, 4, 39, 7, 3, 1, 55, 2, 44]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoM4ivNIEVgW"
      },
      "source": [
        "Next, we will create a function that pads our data. Our neural network will expect to receive an input array in which all the elements (number lists) are of equal size. Since most of the sentences in our dataset are of varying sizes, we must pass our list into a function that can pad our list to ensure that all the elements have the same size. We will use the 'pad_sequences' function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APeNwgpVSaMG"
      },
      "source": [
        "def pad(x, size=None):\n",
        "  return pad_sequences(x, maxlen = size, padding='post')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhSVqJ_gWGqb"
      },
      "source": [
        "Now that we've created the padding & tokenizing functions, we will preprocess our data and move on to splitting our data into training & testing sets before we begin to build our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74EH22tPSznL"
      },
      "source": [
        "french_numbers, french_tokenizer = tokenize(french_sentences)\n",
        "\n",
        "#english_wordlist & french_wordlist are our English & French dictionaries respectively, where the keys represent words & the values represent the numerical representation of the word.\n",
        "english_wordlist = english_tokenizer.word_index\n",
        "french_wordlist = french_tokenizer.word_index\n",
        "\n",
        "english_numbers = pad(english_numbers)\n",
        "french_numbers = pad(french_numbers)\n",
        "\n",
        "french_numbers = french_numbers.reshape(*french_numbers.shape, 1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDztQLzfuBv-"
      },
      "source": [
        "The decode function will take in an array of integers & the language's respective dictionary, and return the sentence translated to it's original form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9lXRJGla_f6"
      },
      "source": [
        "def decode(x, vocab):\n",
        "  translated_sentence = ''\n",
        "  for num in x:\n",
        "    if (num in vocab.values()):\n",
        "      translated_sentence += list(vocab.keys())[list(vocab.values()).index(num)]\n",
        "      translated_sentence += ' '\n",
        "  return translated_sentence"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzqM8hLz2End"
      },
      "source": [
        "Now we will split our input data (english sentences) & our target data (french translations) into a training & testing set. We will use a train-test split of 80:20, and we will later use a validation set from the training data of 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyMrixSvsvEA"
      },
      "source": [
        "#Since both datasets have equal size\n",
        "size = len(english_numbers)\n",
        "\n",
        "#We use a ratio of 0.8:0.2 for the train-test-split\n",
        "train_x = english_numbers[0:math.ceil(size*0.8)]\n",
        "test_x = english_numbers[math.ceil(size*0.8):]\n",
        "train_y = french_numbers[0:math.ceil(size*0.8)]\n",
        "test_y = french_numbers[math.ceil(size*0.8):]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzFkUCJCvAcI"
      },
      "source": [
        "## Building our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek1m-Nsd5mMq",
        "outputId": "0cdea66c-a407-4c04-e198-e1e4696bdc75"
      },
      "source": [
        "x_shape = english_numbers.shape\n",
        "x_shape[1:]\n",
        "#shpe"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZbqOpHeb8O2"
      },
      "source": [
        "from keras.layers import RepeatVector, TimeDistributed"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKg-pAxyczys",
        "outputId": "57a90bdb-65f6-4c7c-f312-8fc80dcd2353"
      },
      "source": [
        "english_numbers.shape[1:]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXGKYGNkuktl"
      },
      "source": [
        "Now we will build our sequential model. The structure of this model is based on the encoder-decoder model for neural machine translation. I have added a reference to an article that discusses the Encoder-decoder model in depth. We will create an input embedding layer, which will take in the vocabulary size of our input (english vocabulary size) and an input size & shape of our english sentences. Next, we will build our encoder layers by adding a bidirectional LSTM, which will be helpful in detecting backward & forward sequences in our data. Lastly, We will build our decoder layers by creating another bidirectional LSTM with 'return_sequences=True' to ensure that the input going into the time distributed layer is of the appropriate proportions. Our output layer will be a time distributed dense layer that takes in the vocabulary size of our output (french vocabulary size). The time distributed layer will simplify the network by requiring far fewer weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9cmJBaXbySv"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# Embedding input layer\n",
        "model.add(Embedding(len(english_wordlist)+1, 128, input_length=english_numbers.shape[1],\n",
        "                         input_shape=english_numbers.shape[1:]))\n",
        "# Encoder\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "model.add(RepeatVector(french_numbers.shape[1]))\n",
        "\n",
        "# Decoder\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(french_wordlist)+1, activation='softmax')))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzgURmuHV6DO",
        "outputId": "809ded48-e022-43ac-eae2-a8127c0177e5"
      },
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 15, 128)           25600     \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 256)               263168    \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 21, 256)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 21, 256)           394240    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 21, 345)           88665     \n",
            "=================================================================\n",
            "Total params: 771,673\n",
            "Trainable params: 771,673\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJSoX2OKWFgL",
        "outputId": "2e8458c6-254f-48c6-dfe4-ee08c21fa1b7"
      },
      "source": [
        "#train_y = train_y.reshape(*train_y.shape, 1)\n",
        "model.fit(train_x, train_y, batch_size=1024, epochs=25, validation_split=0.2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "87/87 [==============================] - 209s 2s/step - loss: 3.8684 - accuracy: 0.4009 - val_loss: 2.3308 - val_accuracy: 0.4948\n",
            "Epoch 2/25\n",
            "87/87 [==============================] - 202s 2s/step - loss: 2.2226 - accuracy: 0.5048 - val_loss: 1.8422 - val_accuracy: 0.5420\n",
            "Epoch 3/25\n",
            "87/87 [==============================] - 202s 2s/step - loss: 1.8144 - accuracy: 0.5469 - val_loss: 1.5951 - val_accuracy: 0.5850\n",
            "Epoch 4/25\n",
            "87/87 [==============================] - 201s 2s/step - loss: 1.5479 - accuracy: 0.5933 - val_loss: 1.4017 - val_accuracy: 0.6259\n",
            "Epoch 5/25\n",
            "87/87 [==============================] - 201s 2s/step - loss: 1.3761 - accuracy: 0.6300 - val_loss: 1.3013 - val_accuracy: 0.6410\n",
            "Epoch 6/25\n",
            "87/87 [==============================] - 201s 2s/step - loss: 1.2515 - accuracy: 0.6572 - val_loss: 1.1791 - val_accuracy: 0.6717\n",
            "Epoch 7/25\n",
            "87/87 [==============================] - 201s 2s/step - loss: 1.1410 - accuracy: 0.6862 - val_loss: 1.0861 - val_accuracy: 0.6962\n",
            "Epoch 8/25\n",
            "87/87 [==============================] - 200s 2s/step - loss: 1.0587 - accuracy: 0.7071 - val_loss: 1.0154 - val_accuracy: 0.7207\n",
            "Epoch 9/25\n",
            "87/87 [==============================] - 199s 2s/step - loss: 0.9989 - accuracy: 0.7205 - val_loss: 0.9367 - val_accuracy: 0.7363\n",
            "Epoch 10/25\n",
            "87/87 [==============================] - 204s 2s/step - loss: 0.9257 - accuracy: 0.7388 - val_loss: 0.8788 - val_accuracy: 0.7511\n",
            "Epoch 11/25\n",
            "87/87 [==============================] - 203s 2s/step - loss: 0.8683 - accuracy: 0.7520 - val_loss: 0.8593 - val_accuracy: 0.7563\n",
            "Epoch 12/25\n",
            "87/87 [==============================] - 203s 2s/step - loss: 0.8273 - accuracy: 0.7616 - val_loss: 0.7954 - val_accuracy: 0.7734\n",
            "Epoch 13/25\n",
            "87/87 [==============================] - 201s 2s/step - loss: 0.7712 - accuracy: 0.7772 - val_loss: 0.7329 - val_accuracy: 0.7896\n",
            "Epoch 14/25\n",
            "87/87 [==============================] - 201s 2s/step - loss: 0.7198 - accuracy: 0.7912 - val_loss: 0.6962 - val_accuracy: 0.7972\n",
            "Epoch 15/25\n",
            "87/87 [==============================] - 202s 2s/step - loss: 0.6694 - accuracy: 0.8047 - val_loss: 0.6496 - val_accuracy: 0.8102\n",
            "Epoch 16/25\n",
            "87/87 [==============================] - 202s 2s/step - loss: 0.6256 - accuracy: 0.8169 - val_loss: 0.6072 - val_accuracy: 0.8228\n",
            "Epoch 17/25\n",
            "87/87 [==============================] - 205s 2s/step - loss: 0.5757 - accuracy: 0.8317 - val_loss: 0.5538 - val_accuracy: 0.8369\n",
            "Epoch 18/25\n",
            "87/87 [==============================] - 205s 2s/step - loss: 0.5268 - accuracy: 0.8460 - val_loss: 0.4996 - val_accuracy: 0.8556\n",
            "Epoch 19/25\n",
            "87/87 [==============================] - 203s 2s/step - loss: 0.4874 - accuracy: 0.8581 - val_loss: 0.4820 - val_accuracy: 0.8608\n",
            "Epoch 20/25\n",
            "87/87 [==============================] - 200s 2s/step - loss: 0.4566 - accuracy: 0.8692 - val_loss: 0.4378 - val_accuracy: 0.8751\n",
            "Epoch 21/25\n",
            "87/87 [==============================] - 201s 2s/step - loss: 0.4166 - accuracy: 0.8822 - val_loss: 0.4023 - val_accuracy: 0.8886\n",
            "Epoch 22/25\n",
            "87/87 [==============================] - 201s 2s/step - loss: 0.3851 - accuracy: 0.8927 - val_loss: 0.3837 - val_accuracy: 0.8930\n",
            "Epoch 23/25\n",
            "87/87 [==============================] - 202s 2s/step - loss: 0.3625 - accuracy: 0.8992 - val_loss: 0.3530 - val_accuracy: 0.9036\n",
            "Epoch 24/25\n",
            "87/87 [==============================] - 204s 2s/step - loss: 0.3376 - accuracy: 0.9072 - val_loss: 0.3322 - val_accuracy: 0.9101\n",
            "Epoch 25/25\n",
            "87/87 [==============================] - 201s 2s/step - loss: 0.3168 - accuracy: 0.9136 - val_loss: 0.3818 - val_accuracy: 0.8910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fed9992d510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF7Vu8spF8iD"
      },
      "source": [
        "Now that our model is trained, we can test it's accuracy by predicting the french translation of english sentences in our test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_9tlCpxFiTK"
      },
      "source": [
        "eng_sentence = decode(test_x[0], english_wordlist)\n",
        "frn_sentence = decode(test_y[0], french_wordlist)\n",
        "result = model.predict(test_x[0])\n",
        "print('English Sentence: ',eng_sentence)\n",
        "print('French Translation prediction: ',eng_sentence)\n",
        "print('Actual French Translation: ',eng_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEOP8uFl2vcL"
      },
      "source": [
        "## Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak6UHiov24XH"
      },
      "source": [
        "I will link some resources that have helped me out in this project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAmjbFLi3OKM"
      },
      "source": [
        "https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation/\n",
        "https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKO0jWnxdBxg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}